#
# Copyright 2022 Benjamin Kiessling
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
# or implied. See the License for the specific language governing
# permissions and limitations under the License.
"""
party.cli.test
~~~~~~~~~~~~~~

Command line driver for recognition training.
"""
import click
import logging

from typing import List

from .util import _expand_gt, _validate_manifests, to_ptl_device

from party.default_specs import RECOGNITION_HYPER_PARAMS

logging.captureWarnings(True)
logger = logging.getLogger('party')

# suppress worker seeding message
logging.getLogger("lightning.fabric.utilities.seed").setLevel(logging.ERROR)


@click.command('test')
@click.pass_context
@click.option('-B', '--batch-size', show_default=True, type=click.INT,
              default=RECOGNITION_HYPER_PARAMS['batch_size'], help='Batch sample size')
@click.option('-m', '--load-from-repo',
              default=None,
              show_default=True,
              help="HTRMoPo identifier of the party model to evaluate")
@click.option('-i', '--load-from-file',
              default=None,
              show_default=True,
              help="Path to the party model to evaluate")
@click.option('-e', '--evaluation-files', show_default=True, default=None, multiple=True,
              callback=_validate_manifests, type=click.File(mode='r', lazy=True),
              help='File(s) with paths to evaluation data.')
@click.option('--workers', show_default=True, default=1,
              type=click.IntRange(0),
              help='Number of worker processes when running on CPU.')
@click.option('-u', '--normalization', show_default=True, type=click.Choice(['NFD', 'NFKD', 'NFC', 'NFKC']),
              default=None, help='Ground truth normalization')
@click.option('-n', '--normalize-whitespace/--no-normalize-whitespace',
              show_default=True, default=True, help='Normalizes unicode whitespace')
@click.option('--curves/--boxes', help='Encode line prompts as bounding boxes or curves', default=None, show_default=True)
@click.option('--compile/--no-compile', help='Switch to enable/disable torch.compile() on model', default=True, show_default=True)
@click.option('--quantize/--no-quantize', help='Switch to enable/disable PTQ', default=False, show_default=True)
@click.option('--add-lang-token/--no-lang-token', help='Switch to enable '
              'language tokens. The token value will be generated by traversing '
              'the path of each file until a component that matches a language '
              'descriptor.', default=False, show_default=True)
@click.argument('test_set', nargs=-1, callback=_expand_gt, type=click.Path(exists=False, dir_okay=False))
def test(ctx, batch_size, load_from_repo, load_from_file, evaluation_files,
         workers, normalization, normalize_whitespace, curves,
         compile, quantize, add_lang_token, test_set):
    """
    Tests a model on a compiled dataset.
    """
    if load_from_file and load_from_repo:
        raise click.BadOptionUsage('load_from_file', 'load_from_* options are mutually exclusive.')
    elif load_from_file is None and load_from_repo is None:
        load_from_repo = '10.5281/zenodo.14616981'

    try:
        accelerator, device = to_ptl_device(ctx.meta['device'])
    except Exception as e:
        raise click.BadOptionUsage('device', str(e))

    import torch

    from htrmopo import get_model
    from collections import defaultdict
    from threadpoolctl import threadpool_limits
    from lightning.fabric import Fabric
    from torch.utils.data import DataLoader

    try:
        from kraken.lib.progress import KrakenProgressBar, KrakenDownloadProgressBar
    except ImportError:
        raise click.UsageError('Inference requires the kraken package')

    from torchmetrics.text import CharErrorRate, WordErrorRate
    from torchmetrics.aggregation import MeanMetric

    from party.fusion import PartyModel
    from party.pred import batched_test_pred
    from party.report import render_report, global_align, compute_script_cer_from_algn
    from party.dataset import TestBaselineDataset
    torch.set_float32_matmul_precision('medium')

    if load_from_repo:
        with KrakenDownloadProgressBar() as progress:
            download_task = progress.add_task(f'Downloading {load_from_repo}', total=0, visible=True)
            p = get_model(load_from_repo,
                          callback=lambda total, advance: progress.update(download_task, total=total, advance=advance))
            load_from_file = next(p.glob('*.safetensors'))

    if curves is True:
        curves = 'curves'
    elif curves is False:
        curves = 'boxes'

    # torchao expects bf16 weights
    if quantize:
        ctx.meta['precision'] = 'bf16-true'

    logger.info('Building test set from {} line images'.format(len(test_set) + len(evaluation_files)))

    test_set = list(test_set)

    if evaluation_files:
        test_set.extend(evaluation_files)

    fabric = Fabric(accelerator=accelerator,
                    devices=device,
                    precision=ctx.meta['precision'])

    ds = TestBaselineDataset(files=test_set,
                             prompt_mode=curves)
    dl = DataLoader(ds, collate_fn=lambda x: x[0], num_workers=workers)

    with torch.inference_mode(), threadpool_limits(limits=ctx.meta['threads']), fabric.init_tensor(), fabric.init_module():

        model = PartyModel.from_safetensors(load_from_file)

        if compile:
            click.echo('Compiling model ', nl=False)
            try:
                model = torch.compile(model, mode='max-autotune')
                click.secho('\u2713', fg='green')
            except Exception:
                click.secho('\u2717', fg='red')

        if quantize:
            click.echo('Quantizing model ', nl=False)
            import torchao
            torchao.quantization.utils.recommended_inductor_config_setter()

            click.secho('\u2713', fg='green')

        algn_gt: List[str] = []
        algn_pred: List[str] = []

        # overall metrics
        micro_test_cer = CharErrorRate()
        micro_test_wer = WordErrorRate()

        page_macro_test_cer = MeanMetric()
        page_macro_test_wer = MeanMetric()

        per_lang_micro_cer = defaultdict(CharErrorRate)
        per_lang_micro_wer = defaultdict(WordErrorRate)

        per_lang_page_macro_cer = defaultdict(MeanMetric)
        per_lang_page_macro_wer = defaultdict(MeanMetric)

        per_script_page_macro_cer = defaultdict(MeanMetric)

        with KrakenProgressBar() as progress:
            file_prog = progress.add_task('Files', total=len(ds))
            for sample in dl:
                try:
                    rec_prog = progress.add_task('Processing sample')
                    im = sample[0]
                    bounds = sample[1]
                    progress.update(rec_prog, total=len(bounds.lines))
                    predictor = batched_test_pred(model=model,
                                                  im=im,
                                                  bounds=bounds,
                                                  fabric=fabric,
                                                  batch_size=batch_size,
                                                  add_lang_token=add_lang_token)

                    page_cer = CharErrorRate()
                    page_wer = WordErrorRate()

                    lang_page_cer = defaultdict(CharErrorRate)
                    lang_page_wer = defaultdict(WordErrorRate)

                    page_algn_gt: List[str] = []
                    page_algn_pred: List[str] = []

                    for pred, line in zip(predictor, bounds.lines):
                        x = pred.prediction
                        y = line.text
                        logger.info(f'pred: {x}\ngt: {y}')
                        algn_s_gt, algn_s_pred = global_align(x, y)
                        page_algn_gt.extend(algn_s_gt)
                        page_algn_pred.extend(algn_s_pred)
                        micro_test_cer.update(x, y)
                        micro_test_wer.update(x, y)
                        page_cer.update(x, y)
                        page_wer.update(x, y)
                        if bounds.language:
                            for lang in bounds.language:
                                per_lang_micro_cer[lang].update(x, y)
                                per_lang_micro_wer[lang].update(x, y)
                                lang_page_cer[lang].update(x, y)
                                lang_page_wer[lang].update(x, y)
                        progress.update(rec_prog, advance=1, total=len(bounds.lines))
                    algn_gt.extend(page_algn_gt)
                    algn_pred.extend(page_algn_pred)

                    for k, v in compute_script_cer_from_algn(algn_gt, algn_pred).items():
                        per_script_page_macro_cer[k].update(v)

                    for lang in lang_page_cer.keys():
                        per_lang_page_macro_cer[lang].update(lang_page_cer[lang].compute())
                        per_lang_page_macro_wer[lang].update(lang_page_wer[lang].compute())
                    page_macro_test_cer.update(page_cer.compute())
                    page_macro_test_wer.update(page_wer.compute())
                except Exception:
                    logger.warning('Sample failed to process.')
                finally:
                    progress.remove_task(rec_prog)
                progress.update(file_prog, advance=1)

        per_lang_micro_cer = {k: float(v.compute()) for k, v in per_lang_micro_cer.items()}
        per_lang_micro_wer = {k: float(v.compute()) for k, v in per_lang_micro_wer.items()}

        for lang in per_lang_micro_cer.keys():
            per_lang_page_macro_cer[lang] = float(per_lang_page_macro_cer[lang].compute())
            per_lang_page_macro_wer[lang] = float(per_lang_page_macro_wer[lang].compute())

        per_script_cer = compute_script_cer_from_algn(algn_gt, algn_pred)

        for k, v in per_script_page_macro_cer.items():
            per_script_page_macro_cer[k] = float(v.compute())

        render_report(load_from_file,
                      float(micro_test_cer.compute()),
                      float(micro_test_wer.compute()),
                      float(page_macro_test_cer.compute()),
                      float(page_macro_test_wer.compute()),
                      per_lang_micro_cer,
                      per_lang_micro_wer,
                      per_lang_page_macro_cer,
                      per_lang_page_macro_wer,
                      per_script_cer,
                      per_script_page_macro_cer)
